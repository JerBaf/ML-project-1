# ML-project-1
First project for the course CS-433 of EPFL.

As the number of jets is determinant in the features that are defined, then there is probably a proper structure for every possible value of the PRI_jet_num feature. First we proceed to mean imputation on the first feature, because we don’t want to handle meaningless values of -999. Which means that we replace every undefined datapoints -999 by the mean of the defined ones. Then we split our data in three distinct partitions based on the number of jets (PRI_jet_num). One bin for the case with zero jet, one bin for the case with a single jet and one last bin for cases with multiple jets. All the next steps, from feature engineering to model training will be made bin wise, i.e. each bin will be treated independently of each other. 
For feature engineering, we have three possible stages (0,1 or 2). Each of the stages can be specified in our script run.py, and will produce different results. The first stage (stage 0) will only take the raw data bins and not apply any processing.
The stage 1 will first perform feature selection based on Pearson correlation and mutual information. We computed these two quantities between each feature and the labels. The use of mutual information is a nice complement to the Pearson correlation because it allows us to capture some non linear dependencies. It is computed from the frequencies of the data, which is an approximation of the probability distribution. This implementation suffers from the choice of the number of bins when we compute the frequencies and thus is not perfectly accurate and is more here to give an idea of the dependencies. We found some matching results between the two quantities which motivated the selection of some particular features for each data bin. We also paid attention not to keep some too much correlated feature as it would reduce the explanatory power of the model.
Finally, stage 2 will perform some feature expansion. It will add some polynomial version of some features and add some joint-products. If a feature’s degree improves the accuracy we add it to our feature list. Same thing for the joint-products. We computed every product of two different features and then took the 6 products of pairs that gave us the best improvement in accuracy.
The results of any stage are then normalized by the mean and standard deviation of the training set. 
The data is now ready to be used for model fitting.
